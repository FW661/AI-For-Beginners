{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training RL to do Cartpole Balancing\n",
    "\n",
    "This notebooks is part of [AI for Beginners Curriculum](http://aka.ms/ai-beginners). It has been inspired by [this blog post](https://medium.com/swlh/policy-gradient-reinforcement-learning-with-keras-57ca6ed32555), [official TensorFlow documentation](https://www.tensorflow.org/tutorials/reinforcement_learning/actor_critic) and [this Keras RL example](https://keras.io/examples/rl/actor_critic_cartpole/).\n",
    "\n",
    "In this example, we will use RL to train a model to balance a pole on a cart that can move left and right on horizontal scale. We will use [OpenAI Gym](https://www.gymlibrary.ml/) environment to simulate the pole.\n",
    "\n",
    "> **Note**: You can run this lesson's code locally (eg. from Visual Studio Code), in which case the simulation will open in a new window. When running the code online, you may need to make some tweaks to the code, as described [here](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).\n",
    "\n",
    "We will start by making sure Gym is installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gym in /home/leo/.local/lib/python3.10/site-packages (0.25.0)\n",
      "Collecting pygame\n",
      "  Downloading pygame-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.9/21.9 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cloudpickle>=1.2.0 in /home/leo/.local/lib/python3.10/site-packages (from gym) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/lib/python3/dist-packages (from gym) (1.21.5)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/leo/.local/lib/python3.10/site-packages (from gym) (0.0.7)\n",
      "Installing collected packages: pygame\n",
      "Successfully installed pygame-2.1.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the CartPole environment and see how to operate on it. An environment has the following properties:\n",
    "\n",
    "* **Action space** is the set of possible actions that we can perform at each step of the simulation\n",
    "* **Observation space** is the space of observations that we can make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(2)\n",
      "Observation space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/.local/lib/python3.10/site-packages/gym/core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/leo/.local/lib/python3.10/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import pygame\n",
    "import tqdm\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the simulation works. The following loop runs the simulation, until `env.step` does not return the termination flag `done`. We will randomly chose actions using `env.action_space.sample()`, which means the experiment will probably fail very fast (CartPole environment terminates when the speed of CartPole, its position or angle are outside certain limits).\n",
    "\n",
    "> Simulation will open in the new window. You can run the code several times and see how it behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/.local/lib/python3.10/site-packages/gym/core.py:57: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
      "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01453476 -0.23855041 -0.04445581  0.29491952] -> 1.0\n",
      "[-0.01930577 -0.4330113  -0.03855742  0.57325697] -> 1.0\n",
      "[-0.027966   -0.62757206 -0.02709228  0.8535481 ] -> 1.0\n",
      "[-0.04051744 -0.43209147 -0.01002132  0.55247074] -> 1.0\n",
      "[-0.04915927 -0.23683023  0.00102809  0.25664735] -> 1.0\n",
      "[-0.05389587 -0.04172298  0.00616104 -0.03571114] -> 1.0\n",
      "[-0.05473033  0.15331008  0.00544682 -0.32644385] -> 1.0\n",
      "[-0.05166413  0.34835407 -0.00108206 -0.6174041 ] -> 1.0\n",
      "[-0.04469705  0.15324724 -0.01343014 -0.3250622 ] -> 1.0\n",
      "[-0.0416321   0.34855783 -0.01993139 -0.62195   ] -> 1.0\n",
      "[-0.03466095  0.54395235 -0.03237038 -0.92084295] -> 1.0\n",
      "[-0.0237819   0.7394964  -0.05078724 -1.2235206 ] -> 1.0\n",
      "[-0.00899197  0.5450641  -0.07525766 -0.9471733 ] -> 1.0\n",
      "[ 0.00190931  0.7411144  -0.09420113 -1.2625213 ] -> 1.0\n",
      "[ 0.0167316   0.54731464 -0.11945155 -1.0007646 ] -> 1.0\n",
      "[ 0.02767789  0.74381286 -0.13946684 -1.3284469 ] -> 1.0\n",
      "[ 0.04255415  0.55069894 -0.16603577 -1.0824591 ] -> 1.0\n",
      "[ 0.05356812  0.35811067 -0.18768495 -0.84614   ] -> 1.0\n",
      "[ 0.06073034  0.55522907 -0.20460775 -1.1914812 ] -> 1.0\n",
      "[ 0.07183492  0.7523274  -0.22843738 -1.5406975 ] -> 1.0\n",
      "Total reward: 20.0\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info = env.step(env.action_space.sample())\n",
    "   total_reward += rew\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "print(f\"Total reward: {total_reward}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Youn can notice that observations contain 4 numbers. They are:\n",
    "- Position of cart\n",
    "- Velocity of cart\n",
    "- Angle of pole\n",
    "- Rotation rate of pole\n",
    "\n",
    "`rew` is the reward we receive at each step. You can see that in CartPole environment you are rewarded 1 point for each simulation step, and the goal is to maximize total reward, i.e. the time CartPole is able to balance without falling.\n",
    "\n",
    "During reinforcement learning, our goal is to train a **policy** $\\pi$, that for each state $s$ will tell us which action $a$ to take, so essentially $a = \\pi(s)$.\n",
    "\n",
    "If you want probabilistic solution, you can think of policy as returning a set of probabilities for each action, i.e. $\\pi(a|s)$ would mean a probability that we should take action $a$ at state $s$.\n",
    "\n",
    "## Policy Gradient Method\n",
    "\n",
    "In simplest RL algorithm, called **Policy Gradient**, we will train a neural network to predict the next action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tensorflow/__init__.py:29: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  import distutils as _distutils\n",
      "2022-07-24 16:22:33.879054: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-24 16:22:33.879079: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/usr/local/lib/python3.10/dist-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "2022-07-24 16:22:35.795765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-24 16:22:35.795964: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-24 16:22:35.796020: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-24 16:22:35.796070: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-24 16:22:35.796120: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-07-24 16:22:35.796169: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-07-24 16:22:35.796218: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-24 16:22:35.796267: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-24 16:22:35.796317: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-07-24 16:22:35.796325: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-07-24 16:22:35.796558: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(128, activation=\"relu\",input_shape=(num_inputs,)),\n",
    "    keras.layers.Dense(num_actions, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train the network by running many experiments, and updating our network after each run. Let's define a function that will run the experiment and return the results (so-called **trace**) - all states, actions (and their recommended probabilities), and rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 10000,render=False):    \n",
    "    states, actions, probs, rewards = [],[],[],[]\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs = model(np.expand_dims(state,0))[0]\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "        nstate, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(action_probs)\n",
    "        rewards.append(reward)\n",
    "        state = nstate\n",
    "    return np.vstack(states), np.vstack(actions), np.vstack(probs), np.vstack(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run one episode with untrained network and observe that total reward (AKA length of episode) is very low:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 27.0\n"
     ]
    }
   ],
   "source": [
    "s,a,p,r = run_episode()\n",
    "print(f\"Total reward: {np.sum(r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the tricky aspects of policy gradient algorithm is to use **discounted rewards**. The idea is that we compute the vector of total rewards at each step of the game, and during this process we discount the early rewards using some coefficient $gamma$. We also normalize the resulting vector, because we will use it as weight to affect our training: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0001\n",
    "\n",
    "def discounted_rewards(rewards,gamma=0.99,normalize=True):\n",
    "    ret = []\n",
    "    s = 0\n",
    "    for r in rewards[::-1]:\n",
    "        s = r + gamma * s\n",
    "        ret.insert(0, s)\n",
    "    if normalize:\n",
    "        ret = (ret-np.mean(ret))/(np.std(ret)+eps)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the actual training! We will run 300 episodes, and at each episode we will do the following:\n",
    "\n",
    "1. Run the experiment and collect the trace\n",
    "1. Calculate the difference (`gradients`) between the actions taken, and by predicted probabilities. The less the difference is, the more we are sure that we have taken the right action.\n",
    "1. Calculate discounted rewards and multiply gradients by discounted rewards - that will make sure that steps with higher rewards will make more effect on the final result than lower-rewarded ones\n",
    "1. Expected target actions for our neural network would be partly taken from the predicted probabilities during the run, and partly from calculated gradients. We will use `alpha` parameter to determine to which extent gradients and rewards are taken into account - this is called *learning rate* of reinforcement algorithm.\n",
    "1. Finally, we train our network on states and expected actions, and repeat the process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> 220.0\n",
      "100 -> 499.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-24 16:24:08.306764: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-07-24 16:24:18.603528: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 -> 499.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-24 16:25:03.522510: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-07-24 16:25:08.844014: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe3b268a650>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha = 1e-4\n",
    "\n",
    "history = []\n",
    "for epoch in range(300):\n",
    "    states, actions, probs, rewards = run_episode()\n",
    "    one_hot_actions = np.eye(2)[actions.T][0]\n",
    "    gradients = one_hot_actions-probs\n",
    "    dr = discounted_rewards(rewards)\n",
    "    gradients *= dr\n",
    "    target = alpha*np.vstack([gradients])+probs\n",
    "    model.train_on_batch(states,target)\n",
    "    history.append(np.sum(rewards))\n",
    "    if epoch%100==0:\n",
    "        print(f\"{epoch} -> {np.sum(rewards)}\")\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the episode with rendering to see the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_37078/1459719159.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_37078/3855001447.py\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(max_steps_per_episode, render)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_steps_per_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0maction_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 )\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mrender_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;34m\"\"\"Renders the environment.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 )\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mrender_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;34m\"set `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             )\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 )\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mrender_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;34m\"\"\"Renders the environment.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 )\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mrender_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/gym/wrappers/env_checker.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_render_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 )\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mrender_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"human\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msurf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msurf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msurf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"human\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: display Surface quit"
     ]
    }
   ],
   "source": [
    "_ = run_episode(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you can see that pole can now balance pretty well!\n",
    "\n",
    "## Actor-Critic Model\n",
    "\n",
    "Actor-Critic model is the further development of policy gradients, in which we build a neural network to learn both the policy and estimated rewards. The network will have two outputs (or you can view it as two separate networks):\n",
    "* **Actor** will recommend the action to take by giving us the state probability distribution, as in policy gradient model\n",
    "* **Critic** would estimate what the reward would be from those actions. It returns total estimated rewards in the future at the given state.\n",
    "\n",
    "Let's define such a model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "num_hidden = 128\n",
    "\n",
    "inputs = keras.layers.Input(shape=(num_inputs,))\n",
    "common = keras.layers.Dense(num_hidden, activation=\"relu\")(inputs)\n",
    "action = keras.layers.Dense(num_actions, activation=\"softmax\")(common)\n",
    "critic = keras.layers.Dense(1)(common)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=[action, critic])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would need to slightly modify our `run_episode` function to return also critic results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_steps_per_episode = 10000,render=False):    \n",
    "    states, actions, probs, rewards, critic = [],[],[],[],[]\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs, est_rew = model(np.expand_dims(state,0))\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs[0]))\n",
    "        nstate, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        probs.append(tf.math.log(action_probs[0,action]))\n",
    "        rewards.append(reward)\n",
    "        critic.append(est_rew[0,0])\n",
    "        state = nstate\n",
    "    return states, actions, probs, rewards, critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will run the main training loop. We will use manual network training process by computing proper loss functions and updating network parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running reward: 5.82 at episode 10\n",
      "running reward: 9.43 at episode 20\n",
      "running reward: 10.30 at episode 30\n",
      "running reward: 10.28 at episode 40\n",
      "running reward: 11.00 at episode 50\n",
      "running reward: 13.01 at episode 60\n",
      "running reward: 21.78 at episode 70\n",
      "running reward: 40.54 at episode 80\n",
      "running reward: 73.70 at episode 90\n",
      "running reward: 100.19 at episode 100\n",
      "running reward: 159.20 at episode 110\n",
      "Solved at episode 114!\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "huber_loss = keras.losses.Huber()\n",
    "episode_count = 0\n",
    "running_reward = 0\n",
    "\n",
    "while True:  # Run until solved\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        _,_,action_probs, rewards, critic_values = run_episode()\n",
    "        episode_reward = np.sum(rewards)\n",
    "        \n",
    "        # Update running reward to check condition for solving\n",
    "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "        # Calculate discounted rewards that will be labels for our critic\n",
    "        dr = discounted_rewards(rewards)\n",
    "\n",
    "        # Calculating loss values to update our network\n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "        for log_prob, value, rew in zip(action_probs, critic_values, dr):\n",
    "            # When we took the action with probability `log_prob`, we received discounted reward of `rew`,\n",
    "            # while critic predicted it to be `value` \n",
    "            # First we calculate actor loss, to make actor predict actions that lead to higher rewards\n",
    "            diff = rew - value\n",
    "            actor_losses.append(-log_prob * diff)\n",
    "\n",
    "            # The critic loss is to minimize the difference between predicted reward `value` and actual\n",
    "            # discounted reward `rew`\n",
    "            critic_losses.append(\n",
    "                huber_loss(tf.expand_dims(value, 0), tf.expand_dims(rew, 0))\n",
    "            )\n",
    "\n",
    "        # Backpropagation\n",
    "        loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    # Log details\n",
    "    episode_count += 1\n",
    "    if episode_count % 10 == 0:\n",
    "        template = \"running reward: {:.2f} at episode {}\"\n",
    "        print(template.format(running_reward, episode_count))\n",
    "\n",
    "    if running_reward > 195:  # Condition to consider the task solved\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the episode and see how good our model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_episode(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaway\n",
    "\n",
    "We have seen two RL algorithms in this demo: simple policy gradient, and more sophisticated actor-critic. You can see that those algorithms operate with abstract notions of state, action and reward - thus they can be applied to very different environments.\n",
    "\n",
    "Reinforcement learning allows us to learn the best strategy to solve the problem just by looking at the final reward. The fact that we do not need labelled datasets allows us to repeat simulations many times to optimize our models. However, there are still many challenges in RL, which you may learn if you decide to focus more on this interesting area of AI.   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
